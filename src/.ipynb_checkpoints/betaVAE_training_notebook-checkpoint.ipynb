{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4109a940-0e39-4c87-a4a9-d1b14b3c0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam, SGD, RAdam\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "# from model import *\n",
    "from betaVAE import *\n",
    "from read_data import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87817e8c-3ca7-4e2c-829b-d2bf503a833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file: ../configs/betavae_tissues.json\n",
      "Checkpoint: ../checkpoints\n",
      "Seed: 99\n",
      "Log: 0\n",
      "Parallel: None\n"
     ]
    }
   ],
   "source": [
    "# Simulating argparse functionality in a Jupyter Notebook\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.config = '../configs/betavae_tissues.json'  # Example: replace with your default config file\n",
    "        self.checkpoint = \"../../checkpoints\"       # Example: replace with your default checkpoint if any\n",
    "        self.seed = 99\n",
    "        self.log = 0\n",
    "        self.parallel = None\n",
    "\n",
    "# Instantiate the simulated args\n",
    "args = Args()\n",
    "\n",
    "# Access the arguments like this:\n",
    "print(f\"Config file: {args.config}\")\n",
    "print(f\"Checkpoint: {args.checkpoint}\")\n",
    "print(f\"Seed: {args.seed}\")\n",
    "print(f\"Log: {args.log}\")\n",
    "print(f\"Parallel: {args.parallel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee3a1c5-ad53-417a-b475-61c20750f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.config) as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a0fa88-8664-45c3-b995-0dc86b0b0e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path_csv': ['../../RNA/data_for_beta_VAE/GTex_Lung_data_SSL_proteincoding.csv',\n",
       "  '../../RNA/data_for_beta_VAE/GTex_BrainCortex_proteincoding.csv',\n",
       "  '../../RNA/data_for_beta_VAE/GTex_Liver_proteincoding.csv',\n",
       "  '../../RNA/data_for_beta_VAE/GTex_Stomach_proteincoding.csv',\n",
       "  '../../RNA/data_for_beta_VAE/GTex_Pancreas_proteincoding.csv'],\n",
       " 'patch_data_path': '../../Histology/Lung_Patches256x256/',\n",
       " 'img_size': 256,\n",
       " 'max_patch_per_wsi': 100,\n",
       " 'rna_features': 19198,\n",
       " 'weights_decay': 0,\n",
       " 'lr': 5e-05,\n",
       " 'num_epochs': 500,\n",
       " 'n_workers': 4,\n",
       " 'device': 0,\n",
       " 'flag': 'betavae_proteincoding_tissues',\n",
       " 'save_dir': '../checkpoints/betavae_training_tissues/',\n",
       " 'summary_path': '../summaries_betavae_tissues/',\n",
       " 'log_interval': 20,\n",
       " 'bag_size': 40,\n",
       " 'batch_size': 128,\n",
       " 'beta': 0.0005,\n",
       " 'quick': 0,\n",
       " 'optimizer': 'Adam'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b7a890-fa8f-4242-bcf6-08405f8f9d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Config for this experiment \n",
      "\n",
      "{'path_csv': ['../../RNA/data_for_beta_VAE/GTex_Lung_data_SSL_proteincoding.csv', '../../RNA/data_for_beta_VAE/GTex_BrainCortex_proteincoding.csv', '../../RNA/data_for_beta_VAE/GTex_Liver_proteincoding.csv', '../../RNA/data_for_beta_VAE/GTex_Stomach_proteincoding.csv', '../../RNA/data_for_beta_VAE/GTex_Pancreas_proteincoding.csv'], 'patch_data_path': '../../Histology/Lung_Patches256x256/', 'img_size': 256, 'max_patch_per_wsi': 100, 'rna_features': 19198, 'weights_decay': 0, 'lr': 5e-05, 'num_epochs': 500, 'n_workers': 4, 'device': 0, 'flag': 'betavae_proteincoding_tissues', 'save_dir': '../checkpoints/betavae_training_tissues/', 'summary_path': '../summaries_betavae_tissues/', 'log_interval': 20, 'bag_size': 40, 'batch_size': 128, 'beta': 0.0005, 'quick': 0, 'optimizer': 'Adam'}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(10*'-')\n",
    "print('Config for this experiment \\n')\n",
    "print(config)\n",
    "print(10*'-')\n",
    "\n",
    "if 'flag' in config:\n",
    "    args.flag = config['flag']\n",
    "else:\n",
    "    args.flag = 'train_{date:%Y-%m-%d %H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "\n",
    "if not os.path.exists(config['save_dir']):\n",
    "    os.mkdir(config['save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e3b8ea-6e42-486e-af3d-d46933c8af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "path_csv = config['path_csv']\n",
    "rna_features = config['rna_features']\n",
    "batch_size = config.get('batch_size', 64)\n",
    "encoder_checkpoint = config.get('encoder_checkpoint', None)\n",
    "beta = config.get('beta', 2)\n",
    "quick = config.get('quick', 0)\n",
    "opt = config.get('optimizer', 'Adam')\n",
    "\n",
    "print('Loading dataset...')\n",
    "\n",
    "datasets = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    'val': []\n",
    "}\n",
    "\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ade2c4e-6f2f-4f14-bd53-829ecb835eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dfs(train_df, val_df, test_df, labels=False, norm_type='standard'):\n",
    "    def _get_log(x):\n",
    "        # trick to take into account zeros\n",
    "        x = np.log(x.replace(0, np.nan))\n",
    "        return x.replace(np.nan, 0)\n",
    "    # get list of columns to scale\n",
    "    rna_columns = [x for x in train_df.columns if 'rna_' in x]\n",
    "    \n",
    "    \n",
    "    # log transform\n",
    "    train_df[rna_columns] = train_df[rna_columns].apply(_get_log)\n",
    "    val_df[rna_columns] = val_df[rna_columns].apply(_get_log)\n",
    "    test_df[rna_columns] = test_df[rna_columns].apply(_get_log)\n",
    "    \n",
    "    \n",
    "    train_df = train_df[rna_columns+['wsi_file_name']]\n",
    "    val_df = val_df[rna_columns+['wsi_file_name']]\n",
    "    test_df = test_df[rna_columns+['wsi_file_name']]\n",
    "    \n",
    "    rna_values = train_df[rna_columns].values\n",
    "\n",
    "    if norm_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif norm_type == 'minmax':\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    rna_values = scaler.fit_transform(rna_values)\n",
    "\n",
    "    train_df[rna_columns] = rna_values\n",
    "    test_df[rna_columns] = scaler.transform(test_df[rna_columns].values)\n",
    "    val_df[rna_columns] = scaler.transform(val_df[rna_columns].values)\n",
    "\n",
    "    return train_df, val_df, test_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51e6115-dae6-4cf1-90c6-4e697291c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../RNA/data_for_beta_VAE/GTex_Lung_data_SSL_proteincoding.csv\n",
      "../../RNA/data_for_beta_VAE/GTex_BrainCortex_proteincoding.csv\n",
      "../../RNA/data_for_beta_VAE/GTex_Liver_proteincoding.csv\n",
      "../../RNA/data_for_beta_VAE/GTex_Stomach_proteincoding.csv\n",
      "../../RNA/data_for_beta_VAE/GTex_Pancreas_proteincoding.csv\n"
     ]
    }
   ],
   "source": [
    "for id, dataset in enumerate(path_csv):\n",
    "    print(dataset)\n",
    "    df = pd.read_csv(dataset)\n",
    "\n",
    "    df_transposed = df.set_index('gene_id').transpose().reset_index()\n",
    "    df_transposed.rename(columns={'index': 'wsi_file_name'}, inplace=True)\n",
    "\n",
    "    train_df, test_df = train_test_split(df_transposed, test_size=0.2)\n",
    "\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "    train_df, val_df, test_df, scaler = normalize_dfs(train_df, val_df, test_df, norm_type='minmax')\n",
    "\n",
    "    datasets['train'].append(train_df)\n",
    "    datasets['test'].append(test_df)\n",
    "    datasets['val'].append(val_df)\n",
    "    \n",
    "    test_labels = test_labels + ([id] * test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "988bc980-b42b-444c-be3f-63fba5b4215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (1084, 19199)\n",
      "Val shape (274, 19199)\n",
      "Test shape (341, 19199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis00/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/dennis00/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "1084it [00:12, 89.38it/s]\n",
      "274it [00:03, 81.05it/s]\n",
      "341it [00:03, 88.82it/s]\n"
     ]
    }
   ],
   "source": [
    "if(len(datasets['train']) >=2):\n",
    "    train_df = pd.concat([datasets['train'][0], datasets['train'][1]])\n",
    "    val_df = pd.concat([datasets['val'][0], datasets['val'][1]])\n",
    "    test_df = pd.concat([datasets['test'][0], datasets['test'][1]])\n",
    "    for i in range(2, len(datasets['train'])):\n",
    "        train_df = pd.concat([train_df, datasets['train'][i]])\n",
    "        val_df = pd.concat([val_df, datasets['val'][i]])\n",
    "        test_df = pd.concat([test_df, datasets['test'][i]])\n",
    "else:\n",
    "    train_df = datasets['train'][0]\n",
    "    val_df = datasets['val'][0]\n",
    "    test_df = datasets['test'][0]\n",
    "\n",
    "print('Train shape {}'.format(train_df.shape))\n",
    "print('Val shape {}'.format(val_df.shape))\n",
    "print('Test shape {}'.format(test_df.shape))\n",
    "train_df, val_df, test_df, scaler = normalize_dfs(train_df, val_df, test_df, norm_type='standard')\n",
    "\n",
    "train_dataset = RNADataset([train_df], quick=quick)\n",
    "val_dataset = RNADataset([val_df])\n",
    "test_dataset = RNADataset([test_df])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size, \n",
    "               num_workers=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size, \n",
    "               num_workers=16, \n",
    "               shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=1, \n",
    "               num_workers=16, \n",
    "               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6741a54-0fa5-4318-972b-67c431d98b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading dataset and creating dataloader\n",
      "Initializing models\n",
      "Restoring from checkpoint\n",
      "../checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_293291/3920628376.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(args.checkpoint))\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRestoring from checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(args\u001b[38;5;241m.\u001b[39mcheckpoint)\n\u001b[0;32m---> 23\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded model from checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/serialization.py:1327\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1325\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1329\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1332\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/scRNA-GAN-TF/lib/python3.12/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../checkpoints'"
     ]
    }
   ],
   "source": [
    "print('Finished loading dataset and creating dataloader')\n",
    "\n",
    "print('Initializing models')\n",
    "\n",
    "\n",
    "if encoder_checkpoint:\n",
    "    model = betaVAE(rna_features, 2048, [12000, 4096, 2048], [4096, 12000],\n",
    "                      encoder_checkpoint=encoder_checkpoint)\n",
    "    if args.checkpoint is not None:\n",
    "        print('Restoring from checkpoint')\n",
    "        print(args.checkpoint)\n",
    "        model.load_state_dict(torch.load(args.checkpoint))\n",
    "        print('Loaded model from checkpoint')\n",
    "    else:\n",
    "        model.z_mu.apply(init_weights_uniform)\n",
    "        model.decoder.apply(init_weights_uniform)\n",
    "        model.z_logvar.apply(init_weights_uniform)\n",
    "else:\n",
    "    model = betaVAE(rna_features, 2048, [6000, 4000, 2048], [4000, 6000], beta=beta)\n",
    "    if args.checkpoint is not None:\n",
    "        print('Restoring from checkpoint')\n",
    "        print(args.checkpoint)\n",
    "        model.load_state_dict(torch.load(args.checkpoint))\n",
    "        print('Loaded model from checkpoint')\n",
    "    else:\n",
    "        model.apply(init_weights_xavier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
